{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16773159",
   "metadata": {},
   "source": [
    "# Experiment 2: Transfer learning for ConvNP\n",
    "\n",
    "When training the ConvNP for data assimilation, we want to be able to first train the model on a broad dataset, covering a large portion of Australasia. We then want to refine the ConvNP for predictions on a narrower dataset, specifically for New Zealand. To make a plan for what context, auxiliary and target sets we use, we need to know how the default model structure changes to different input structures.\n",
    "\n",
    "Things which need to be considered:\n",
    "- Target resolution/model internal density\n",
    "- Input scale\n",
    "- Number of context sets\n",
    "- Number of target sets\n",
    "- Multi-output vs single-output?\n",
    "\n",
    "Note: if the encoder changes structure but the decoder stays the same, then we might be able to train seperate encoders, but share the decoder structure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f690409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup project root for imports (requirement for all notebooks in this repo)\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Make project root importable\n",
    "ROOT = Path().resolve().parents[1]\n",
    "sys.path.append(str(ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2502d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "281f316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from src.utils.variables.var_names import *\n",
    "from src.utils.variables.coord_names import *\n",
    "from src.data_processing.conversions.scalar_conversions import *\n",
    "from src.config.env_loader import get_env_var\n",
    "import src.learning.model_diagnostics as model_diagnostics\n",
    "from src.learning.model_training import batch_data_by_num_stations, compute_val_loss\n",
    "\n",
    "from src.data_processing.station_processor import ProcessStations\n",
    "from src.data_processing.topography_processor import ProcessTopography\n",
    "from src.data_processing.era5_processor import ProcessERA5\n",
    "\n",
    "from src.data_processing.auxiliary.sun_position import get_sun_culmination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dffbd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepSensor imports\n",
    "# note this pulls from a fork of DeepSensor.\n",
    "import deepsensor.torch\n",
    "from deepsensor.train.train import train_epoch, set_gpu_default_device\n",
    "from deepsensor.data.loader import TaskLoader\n",
    "from deepsensor.data.processor import DataProcessor\n",
    "from deepsensor.model.convnp import ConvNP\n",
    "from deepsensor.data.utils import construct_x1x2_ds\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import torch\n",
    "from torch import optim\n",
    "import os\n",
    "import lab as B\n",
    "from tqdm import tqdm\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aea63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup variables for experiment\n",
    "years = [2010, 2011, 2012, 2013, 2014]\n",
    "\n",
    "train_years = [2010, 2011, 2012, 2013]\n",
    "validation_years = [2014]\n",
    "\n",
    "# GPU settings\n",
    "use_gpu = True\n",
    "if use_gpu:\n",
    "    cuda_device = int(get_env_var(\"CUDA_DEVICE\"))\n",
    "    set_gpu_default_device(backend=\"cuda\", dev_id=cuda_device)\n",
    "\n",
    "# visualisations of data\n",
    "DEBUG_PLOTS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33be307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset loader modules. These are from src.data_processing.\n",
    "#    They use a file loader module in src.data_processing.file_loaders to load raw data files,\n",
    "#    in the case of getting data in a different structure, changes will need to be made to the file loader modules.\n",
    "station_processor = ProcessStations()\n",
    "topography_processor = ProcessTopography()\n",
    "era5_processor = ProcessERA5()\n",
    "\n",
    "# topography and ERA5 datasets are loaded as simple xarray datasets\n",
    "topography_ds = topography_processor.load_ds(standardise_var_names=True, standardise_coord_names=True)\n",
    "era5_ds = era5_processor.load_ds(mode=\"surface\", years=years, standardise_var_names=True, standardise_coord_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d96728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
