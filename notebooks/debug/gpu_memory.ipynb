{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe513ed8",
   "metadata": {},
   "source": [
    "# Debugging GPU Memroy Usage in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87400a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Make project root importable\n",
    "ROOT = Path().resolve().parents[1]\n",
    "sys.path.append(str(ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "405f588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13ae41f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from src.utils.variables.var_names import *\n",
    "from src.utils.variables.coord_names import *\n",
    "from src.data_processing.conversions.scalar_conversions import *\n",
    "from src.config.env_loader import get_env_var\n",
    "import src.learning.model_diagnostics as model_diagnostics\n",
    "from src.learning.model_training import batch_data_by_num_stations, compute_val_loss\n",
    "\n",
    "from src.data_processing.station_processor import ProcessStations\n",
    "from src.data_processing.topography_processor import ProcessTopography\n",
    "from src.data_processing.era5_processor import ProcessERA5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "611867c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "import deepsensor.torch\n",
    "from deepsensor.train.train import train_epoch, set_gpu_default_device\n",
    "from deepsensor.data.loader import TaskLoader\n",
    "from deepsensor.data.processor import DataProcessor\n",
    "from deepsensor.model.convnp import ConvNP\n",
    "from deepsensor.data.utils import construct_x1x2_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b55523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import torch\n",
    "from torch import optim\n",
    "import os\n",
    "import lab as B\n",
    "from tqdm import tqdm\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d625ba5",
   "metadata": {},
   "source": [
    "## Set up a Demo Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55dcf585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_env_var(\"CUDA_DEVICE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5be4d2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d15a149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup variables for experiment\n",
    "var = TEMPERATURE\n",
    "years = [2010, 2011, 2012, 2013, 2014]\n",
    "\n",
    "train_years = [2010] #[2010, 2011, 2012, 2013]\n",
    "validation_years = [2014]\n",
    "\n",
    "# GPU settings\n",
    "use_gpu = True\n",
    "if use_gpu:\n",
    "    cuda_device = int(get_env_var(\"CUDA_DEVICE\"))\n",
    "    set_gpu_default_device(backend=\"cuda\", dev_id=2)\n",
    "\n",
    "# visualisations of data\n",
    "DEBUG_PLOTS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6585c563",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_processor = ProcessStations()\n",
    "topography_processor = ProcessTopography()\n",
    "era5_processor = ProcessERA5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e05416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "topography_ds = topography_processor.load_ds(standardise_var_names=True, standardise_coord_names=True)\n",
    "era5_ds = era5_processor.load_ds(mode=\"surface\", years=years, standardise_var_names=True, standardise_coord_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4c66b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_var = era5_processor.get_variable(era5_ds, var) # set variable to process - e.g. \"temperature\"\n",
    "era5_var = kelvin_to_celsius(era5_var)\n",
    "era5_ds[var] = era5_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93084240",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop = False\n",
    "\n",
    "crop_left = 166\n",
    "crop_right = 176\n",
    "crop_top = -38\n",
    "crop_bottom = -48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b53fe100",
   "metadata": {},
   "outputs": [],
   "source": [
    "if crop:\n",
    "    era5_ds = era5_ds.sel(lat=slice(crop_top, crop_bottom), lon=slice(crop_left, crop_right))\n",
    "    topography_ds = topography_ds.sel(lat=slice(crop_bottom, crop_top), lon=slice(crop_left, crop_right))\n",
    "\n",
    "era5_ds_coarsen = era5_ds.coarsen(lat=5, lon=5, boundary='trim').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e5e2eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_aux = topography_processor.compute_tpi(topography_ds, window_sizes=[0.1])\n",
    "\n",
    "# coarsen the elevation data\n",
    "ds_aux_coarse  = ds_aux.coarsen(lat=200, lon=200, boundary='trim').mean()\n",
    "\n",
    "ds_aux = ds_aux.fillna(0)\n",
    "ds_aux_coarse = ds_aux_coarse.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f410810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crowelenn/niwa/convcnp-assim-nz/src/data_processing/station_processor.py:157: FutureWarning: In a future version of xarray the default value for join will change from join='outer' to join='exact'. This change will result in the following ValueError: cannot be aligned with join='exact' because index/labels/sizes are not equal along these coordinates (dimensions): 'time' ('time',) The recommendation is to set join explicitly for this case.\n",
      "  ds_comb = xr.concat([first, *station_iter], dim=\"station\")\n",
      "/tmp/ipykernel_96774/2471903628.py:5: FutureWarning: DataFrameGroupBy.resample operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  stations_resample = stations_reset.groupby(['lat', 'lon']).resample(\"6h\", on='time').mean()[['temperature']]\n"
     ]
    }
   ],
   "source": [
    "stations_df = station_processor.load_df(vars=[var], year_start=2010, year_end=2014)\n",
    "stations_df.head()\n",
    "stations_reset = stations_df.reset_index()\n",
    "stations_reset.drop(columns=['station'], inplace=True)\n",
    "stations_resample = stations_reset.groupby(['lat', 'lon']).resample(\"6h\", on='time').mean()[['temperature']]\n",
    "stations_resample = stations_resample.reset_index().set_index(['time', 'lat', 'lon']).sort_index()\n",
    "\n",
    "if crop:\n",
    "    stations_resample = stations_resample[(stations_resample.index.get_level_values('lat') > crop_bottom) & (stations_resample.index.get_level_values('lat') < crop_top) &\n",
    "                                      (stations_resample.index.get_level_values('lon') > crop_left) & (stations_resample.index.get_level_values('lon') < crop_right)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af6769b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_da = era5_ds.sel(lat=slice(ds_aux_coarse[LATITUDE].max(), ds_aux_coarse[LATITUDE].min()), lon=slice(ds_aux_coarse[LONGITUDE].min(), ds_aux_coarse[LONGITUDE].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85d3cd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_ds_coarsen = era5_ds_coarsen[[var]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7987461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crowelenn/niwa/convcnp-assim-nz/venv/src/deepsensor/deepsensor/data/processor.py:129: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  f\"x1_map={x1_map} and x2_map={x2_map} have different ranges ({float(np.diff(x1_map))} \"\n",
      "/home/crowelenn/niwa/convcnp-assim-nz/venv/src/deepsensor/deepsensor/data/processor.py:130: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  f\"and {float(np.diff(x2_map))}, respectively). \"\n",
      "/home/crowelenn/niwa/convcnp-assim-nz/venv/src/deepsensor/deepsensor/data/processor.py:128: UserWarning: x1_map=(-50.0, -32.0) and x2_map=(165.0, 180.0) have different ranges (18.0 and 15.0, respectively). This can lead to stretching/squashing of data, which may impact model performance.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data_processor = DataProcessor(x1_name=LATITUDE, x1_map=(era5_ds[LATITUDE].min(), era5_ds[LATITUDE].max()), x2_name=LONGITUDE, x2_map=(era5_ds[LONGITUDE].min(), era5_ds[LONGITUDE].max()))\n",
    "era5_processed, station_processed = data_processor([era5_ds_coarsen, stations_resample])\n",
    "ds_aux_processed, ds_aux_coarse_processed = data_processor([ds_aux, ds_aux_coarse], method='min_max')\n",
    "\n",
    "x1x2_ds = construct_x1x2_ds(ds_aux_coarse_processed)\n",
    "ds_aux_coarse_processed['x1_arr'] = x1x2_ds['x1_arr']\n",
    "ds_aux_coarse_processed['x2_arr'] = x1x2_ds['x2_arr']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372906ed",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ea58884",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_loader = TaskLoader(\n",
    "        context = [station_processed, era5_processed, ds_aux_coarse_processed], \n",
    "        target = station_processed, \n",
    "        aux_at_targets = ds_aux_processed, \n",
    "        links = [(0, 0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12d6b64",
   "metadata": {},
   "source": [
    "### Investigating why the default internal grid is so dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f9ac9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_yc inferred from TaskLoader: (1, 1, 4)\n",
      "dim_yt inferred from TaskLoader: 1\n",
      "dim_aux_t inferred from TaskLoader: 2\n",
      "Setting aux_t_mlp_layers: (64, 64, 64)\n",
      "internal_density inferred from TaskLoader: 6877\n",
      "encoder_scales inferred from TaskLoader: [7.270612185546023e-05, 0.0347222238779068, 0.007111109793186188]\n",
      "decoder_scale inferred from TaskLoader: 0.00014541224371092046\n"
     ]
    }
   ],
   "source": [
    "model = ConvNP(data_processor, task_loader, unet_channels=(64,)*5, likelihood=\"gnp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "689a1752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from deepsensor.data.utils import (\n",
    "    compute_xarray_data_resolution,\n",
    "    compute_pandas_data_resolution,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e3a7a5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "station_processed: data resolution = 6877.478607561156\n",
      "era5_processed: data resolution = 14.400000000000002\n",
      "ds_aux_coarse_processed: data resolution = 70.3125130312425\n"
     ]
    }
   ],
   "source": [
    "variables = {\n",
    "    \"station_processed\": station_processed,\n",
    "    \"era5_processed\": era5_processed,\n",
    "    \"ds_aux_coarse_processed\": ds_aux_coarse_processed,\n",
    "}\n",
    "\n",
    "for name, var in variables.items():\n",
    "    if isinstance(var, (xr.DataArray, xr.Dataset)):\n",
    "        # Gridded variable: use data resolution\n",
    "        data_resolution = compute_xarray_data_resolution(var)\n",
    "    elif isinstance(var, (pd.DataFrame, pd.Series)):\n",
    "        # Point-based variable: calculate density\n",
    "        data_resolution = compute_pandas_data_resolution(\n",
    "            var, n_times=1000, percentile=5\n",
    "        )\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    print(f\"{name}: data resolution = {1/data_resolution}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fbd642",
   "metadata": {},
   "source": [
    "The dataset with the highest internal density is 'station_processed' with an internal density of 6877. This is then selected as the ConvNP internal density.\n",
    "\n",
    "Why is the internal density from the off-grid stations dataset so high?\n",
    "\n",
    "From deepsensor: `# Point-based variable: calculate density based on pairwise distances between observations`\n",
    "\n",
    "`The resolution is approximated as the Nth percentile of the distances\n",
    "    between neighbouring observations, possibly using a subset of the dates in\n",
    "    the data. The default is to use 1000 dates (or all dates if there are fewer\n",
    "    than 1000) and to use the 5th percentile. This means that the resolution is\n",
    "    the distance between the closest 5% of neighbouring observations.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3d32c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the function deepsensor use, but I modified it to also return the list of distances:\n",
    "import scipy\n",
    "def compute_pandas_data_resolution(\n",
    "    df,\n",
    "    n_times = 1000,\n",
    "    percentile = 5,\n",
    "):\n",
    "    \"\"\"Approximates the resolution of non-gridded pandas data with indexes time,\n",
    "    x1, and x2.\n",
    "\n",
    "    The resolution is approximated as the Nth percentile of the distances\n",
    "    between neighbouring observations, possibly using a subset of the dates in\n",
    "    the data. The default is to use 1000 dates (or all dates if there are fewer\n",
    "    than 1000) and to use the 5th percentile. This means that the resolution is\n",
    "    the distance between the closest 5% of neighbouring observations.\n",
    "\n",
    "    Args:\n",
    "        df (:class:`pandas.DataFrame` | :class:`pandas.Series`):\n",
    "            Dataframe or series with indexes time, x1, and x2.\n",
    "        n_times (int, optional):\n",
    "            Number of dates to sample. Defaults to 1000. If \"all\", all dates\n",
    "            are used.\n",
    "        percentile (int, optional):\n",
    "            Percentile of pairwise distances for computing the resolution.\n",
    "            Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        float: Resolution of the data (in spatial units, e.g. 0.1 degrees).\n",
    "    \"\"\"\n",
    "    dates = df.index.get_level_values(\"time\").unique()\n",
    "\n",
    "    if n_times != \"all\" and len(dates) > n_times:\n",
    "        rng = np.random.default_rng(42)\n",
    "        dates = rng.choice(dates, size=n_times, replace=False)\n",
    "\n",
    "    closest_distances = []\n",
    "    df = df.reset_index().set_index(\"time\")\n",
    "    for time in dates:\n",
    "        df_t = df.loc[[time]]\n",
    "        X = df_t[[\"x1\", \"x2\"]].values  # (N, 2) array of coordinates\n",
    "        if X.shape[0] < 2:\n",
    "            # Skip this time if there are fewer than 2 stationS\n",
    "            continue\n",
    "        X_unique = np.unique(X, axis=0)  # (N_unique, 2) array of unique coordinates\n",
    "\n",
    "        pairwise_distances = scipy.spatial.distance.cdist(X_unique, X_unique)\n",
    "        percentile_distances_without_self = np.ma.masked_equal(pairwise_distances, 0)\n",
    "\n",
    "        # Compute the closest distance from each station to each other station\n",
    "        closest_distances_t = np.min(percentile_distances_without_self, axis=1)\n",
    "        closest_distances.extend(closest_distances_t)\n",
    "\n",
    "    data_resolution = np.percentile(closest_distances, percentile)\n",
    "    return data_resolution, closest_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e73d60c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res, dists = compute_pandas_data_resolution(station_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a5df7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00014540212439201073\n"
     ]
    }
   ],
   "source": [
    "# this is the station resolution\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6cbe578f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0001454, 0.0001454, 0.0001454, 0.0001454, 0.0001454, 0.0001454,\n",
       "       0.0001454, 0.0001454, 0.0001454, 0.0001454])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists_sort = np.sort(dists)\n",
    "dists_sort[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b95dc475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{np.float64(0.00014540212439201073),\n",
       " np.float64(0.02263853027664162),\n",
       " np.float64(0.02470115107029566),\n",
       " np.float64(0.026429859896024165),\n",
       " np.float64(0.05185481681686933),\n",
       " np.float64(0.05546184612644111),\n",
       " np.float64(0.06499568582074207),\n",
       " np.float64(0.11388813290242192),\n",
       " np.float64(0.11610188959133654),\n",
       " np.float64(0.16085396807276103)}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dists_sort)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3211e224",
   "metadata": {},
   "source": [
    "There are only 10 different distance values. The 5th percentile will always return 0.000145 as this appears enough times to be the fifth percentile. If 0.02263 was the resolution instead, the internal density would be only 1/0.0226 ~ 44. This would be much more reasonable. There are two stations which are too close together and it is making the internal_density way too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dc85bf2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_i</th>\n",
       "      <th>station_j</th>\n",
       "      <th>distance_km</th>\n",
       "      <th>lat_i</th>\n",
       "      <th>lon_i</th>\n",
       "      <th>lat_j</th>\n",
       "      <th>lon_j</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0.283363</td>\n",
       "      <td>-38.66100</td>\n",
       "      <td>177.98600</td>\n",
       "      <td>-38.65860</td>\n",
       "      <td>177.98513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>44.799918</td>\n",
       "      <td>-38.97352</td>\n",
       "      <td>175.79080</td>\n",
       "      <td>-38.68400</td>\n",
       "      <td>176.07200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>45.186606</td>\n",
       "      <td>-37.00884</td>\n",
       "      <td>174.80713</td>\n",
       "      <td>-36.60268</td>\n",
       "      <td>174.83458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>47.158797</td>\n",
       "      <td>-38.65860</td>\n",
       "      <td>177.98513</td>\n",
       "      <td>-38.38228</td>\n",
       "      <td>178.30785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>47.259449</td>\n",
       "      <td>-38.66100</td>\n",
       "      <td>177.98600</td>\n",
       "      <td>-38.38228</td>\n",
       "      <td>178.30785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>100.389445</td>\n",
       "      <td>-38.97352</td>\n",
       "      <td>175.79080</td>\n",
       "      <td>-38.33174</td>\n",
       "      <td>175.15356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>103.143419</td>\n",
       "      <td>-39.89320</td>\n",
       "      <td>175.65799</td>\n",
       "      <td>-38.97352</td>\n",
       "      <td>175.79080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>109.188074</td>\n",
       "      <td>-38.68400</td>\n",
       "      <td>176.07200</td>\n",
       "      <td>-38.33174</td>\n",
       "      <td>175.15356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>116.072688</td>\n",
       "      <td>-40.57728</td>\n",
       "      <td>176.44889</td>\n",
       "      <td>-39.89320</td>\n",
       "      <td>175.65799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>141.870373</td>\n",
       "      <td>-39.89320</td>\n",
       "      <td>175.65799</td>\n",
       "      <td>-38.68400</td>\n",
       "      <td>176.07200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>151.793389</td>\n",
       "      <td>-38.33174</td>\n",
       "      <td>175.15356</td>\n",
       "      <td>-37.00884</td>\n",
       "      <td>174.80713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>182.141731</td>\n",
       "      <td>-39.89320</td>\n",
       "      <td>175.65799</td>\n",
       "      <td>-38.33174</td>\n",
       "      <td>175.15356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>190.346036</td>\n",
       "      <td>-45.24600</td>\n",
       "      <td>169.38956</td>\n",
       "      <td>-44.97600</td>\n",
       "      <td>171.08300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>192.421904</td>\n",
       "      <td>-40.57728</td>\n",
       "      <td>176.44889</td>\n",
       "      <td>-38.97352</td>\n",
       "      <td>175.79080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>195.164287</td>\n",
       "      <td>-38.33174</td>\n",
       "      <td>175.15356</td>\n",
       "      <td>-36.60268</td>\n",
       "      <td>174.83458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>205.648046</td>\n",
       "      <td>-43.60369</td>\n",
       "      <td>172.64928</td>\n",
       "      <td>-42.46022</td>\n",
       "      <td>171.19157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>212.376145</td>\n",
       "      <td>-38.68400</td>\n",
       "      <td>176.07200</td>\n",
       "      <td>-38.65860</td>\n",
       "      <td>177.98513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>212.469339</td>\n",
       "      <td>-38.68400</td>\n",
       "      <td>176.07200</td>\n",
       "      <td>-38.66100</td>\n",
       "      <td>177.98600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>214.277593</td>\n",
       "      <td>-40.57728</td>\n",
       "      <td>176.44889</td>\n",
       "      <td>-38.68400</td>\n",
       "      <td>176.07200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>231.148479</td>\n",
       "      <td>-44.97600</td>\n",
       "      <td>171.08300</td>\n",
       "      <td>-43.60369</td>\n",
       "      <td>172.64928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    station_i  station_j  distance_km     lat_i      lon_i     lat_j  \\\n",
       "0           9         10     0.283363 -38.66100  177.98600 -38.65860   \n",
       "1           7          8    44.799918 -38.97352  175.79080 -38.68400   \n",
       "2          13         14    45.186606 -37.00884  174.80713 -36.60268   \n",
       "3          10         11    47.158797 -38.65860  177.98513 -38.38228   \n",
       "4           9         11    47.259449 -38.66100  177.98600 -38.38228   \n",
       "5           7         12   100.389445 -38.97352  175.79080 -38.33174   \n",
       "6           6          7   103.143419 -39.89320  175.65799 -38.97352   \n",
       "7           8         12   109.188074 -38.68400  176.07200 -38.33174   \n",
       "8           4          6   116.072688 -40.57728  176.44889 -39.89320   \n",
       "9           6          8   141.870373 -39.89320  175.65799 -38.68400   \n",
       "10         12         13   151.793389 -38.33174  175.15356 -37.00884   \n",
       "11          6         12   182.141731 -39.89320  175.65799 -38.33174   \n",
       "12          0          1   190.346036 -45.24600  169.38956 -44.97600   \n",
       "13          4          7   192.421904 -40.57728  176.44889 -38.97352   \n",
       "14         12         14   195.164287 -38.33174  175.15356 -36.60268   \n",
       "15          2          3   205.648046 -43.60369  172.64928 -42.46022   \n",
       "16          8         10   212.376145 -38.68400  176.07200 -38.65860   \n",
       "17          8          9   212.469339 -38.68400  176.07200 -38.66100   \n",
       "18          4          8   214.277593 -40.57728  176.44889 -38.68400   \n",
       "19          1          2   231.148479 -44.97600  171.08300 -43.60369   \n",
       "\n",
       "        lon_j  \n",
       "0   177.98513  \n",
       "1   176.07200  \n",
       "2   174.83458  \n",
       "3   178.30785  \n",
       "4   178.30785  \n",
       "5   175.15356  \n",
       "6   175.79080  \n",
       "7   175.15356  \n",
       "8   175.65799  \n",
       "9   176.07200  \n",
       "10  174.80713  \n",
       "11  175.15356  \n",
       "12  171.08300  \n",
       "13  175.79080  \n",
       "14  174.83458  \n",
       "15  171.19157  \n",
       "16  177.98513  \n",
       "17  177.98600  \n",
       "18  176.07200  \n",
       "19  172.64928  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_unique = (\n",
    "    stations_resample\n",
    "    .reset_index()[[\"lat\", \"lon\"]]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "coords = stations_unique[[\"lat\", \"lon\"]].to_numpy()\n",
    "\n",
    "dist_matrix = scipy.spatial.distance.cdist(coords, coords, metric=\"euclidean\")  # distances in degrees\n",
    "\n",
    "dist_matrix_km = dist_matrix * 111.0\n",
    "\n",
    "pairs = []\n",
    "\n",
    "n = len(stations_unique)\n",
    "for i in range(n):\n",
    "    for j in range(i + 1, n):\n",
    "        pairs.append((i, j, dist_matrix_km[i, j]))\n",
    "\n",
    "distances = pd.DataFrame(\n",
    "    pairs,\n",
    "    columns=[\"station_i\", \"station_j\", \"distance_km\"]\n",
    ").sort_values(\"distance_km\").reset_index(drop=True)\n",
    "\n",
    "distances[\"lat_i\"] = distances[\"station_i\"].map(stations_unique[\"lat\"])\n",
    "distances[\"lon_i\"] = distances[\"station_i\"].map(stations_unique[\"lon\"])\n",
    "distances[\"lat_j\"] = distances[\"station_j\"].map(stations_unique[\"lat\"])\n",
    "distances[\"lon_j\"] = distances[\"station_j\"].map(stations_unique[\"lon\"])\n",
    "\n",
    "distances.head(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e72e9e",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "387315f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_loader.load_dask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48aac0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dates = era5_ds.sel(time=slice(\"2010-01-01\", \"2011-12-31\")).time.values\n",
    "val_dates = era5_ds.sel(time=slice(\"2012-01-01\", \"2012-06-30\")).time.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eed89d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2920 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2920/2920 [00:58<00:00, 49.52it/s]\n",
      "100%|██████████| 728/728 [00:15<00:00, 46.40it/s]\n"
     ]
    }
   ],
   "source": [
    "train_tasks = []\n",
    "for date in tqdm(train_dates):\n",
    "    task = task_loader(date, context_sampling=[\"split\", \"all\", \"all\"], target_sampling=[\"split\"], split_frac=0.5)\n",
    "    train_tasks.append(task)\n",
    "\n",
    "\n",
    "val_tasks = []\n",
    "for date in tqdm(val_dates):\n",
    "    task = task_loader(date, context_sampling=[\"split\", \"all\", \"all\"], target_sampling=[\"split\"], split_frac=0.5)\n",
    "    val_tasks.append(task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe6ddf65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2010-01-01 06:00:00\n",
      "ops: []\n",
      "X_c: [(2, 7), ((1, 14), (1, 12)), ((1, 54), (1, 54))]\n",
      "Y_c: [(1, 7), (1, 14, 12), (4, 54, 54)]\n",
      "X_t: [(2, 4)]\n",
      "Y_t: [(1, 4)]\n",
      "Y_t_aux: (2, 4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_tasks[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5160a6d1",
   "metadata": {},
   "source": [
    "- three context sets\n",
    "- X_c is the coordinates for the context sets\n",
    "- Y_c is the values for the context sets\n",
    "- 3, 20x20, 78x78 observations (Y_c)\n",
    "- X_t is the target sensor coordinates\n",
    "- Y_t is the target sensor values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c32cf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_batched = batch_data_by_num_stations(train_tasks, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0d8d4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/home/crowelenn/niwa/convcnp-assim-nz/venv/lib/python3.12/site-packages/lab/types.py:204: DeprecationWarning: numpy.core is deprecated and has been renamed to numpy._core. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numerictypes.\n",
      "  for name in np.core.numerictypes.__all__ + [\"bool\"]:\n",
      " 33%|███▎      | 1/3 [00:49<01:39, 49.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train_loss: 1.72, val_loss: 1.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [01:32<00:45, 45.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train_loss: 1.64, val_loss: 1.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [02:14<00:00, 44.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train_loss: 1.61, val_loss: 1.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "lr=5e-5\n",
    "\n",
    "output_model = False\n",
    "\n",
    "val_loss_best = np.inf\n",
    "\n",
    "opt = optim.Adam(model.model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    batch_losses = [train_epoch(model, task_batched[f'{num_stations}'], \n",
    "                                            batch_size=len(task_batched[f'{num_stations}']), \n",
    "                                            lr=lr, opt=opt) for num_stations in task_batched.keys()]\n",
    "    \n",
    "    train_loss = np.mean(batch_losses)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loss = compute_val_loss(model, val_tasks)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    if val_loss < val_loss_best:\n",
    "        val_loss_best = val_loss\n",
    "        if output_model:\n",
    "            folder = os.path.join(get_env_var(\"OUTPUT_HOME\"), \"models\", \"downscaling\", \"temperature\", \"convcnp\")\n",
    "            if not os.path.exists(folder): os.makedirs(folder)\n",
    "            torch.save(model.model.state_dict(), folder + f\"model.pt\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Epoch {epoch} train_loss: {train_loss:.2f}, val_loss: {val_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81affb3c",
   "metadata": {},
   "source": [
    "tensor multiplication: [[16, 16000, 6], [16, 6, 8128]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d8f4a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 9, 64, 64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepsensor.model.nps.compute_encoding_tensor(model, task).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
